{"cells":[{"cell_type":"code","source":["# Install library not packaged in Fabric\n","%pip install ratelimit"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":20,"statement_ids":[15,16,17,18,19,20],"state":"finished","livy_statement_state":"available","session_id":"83d43b1c-1c3b-49ed-92cf-5669aa60253c","normalized_state":"finished","queued_time":"2025-11-06T18:50:55.6637293Z","session_start_time":null,"execution_start_time":"2025-11-06T18:50:56.4373246Z","execution_finish_time":"2025-11-06T18:51:09.3955797Z","parent_msg_id":"d44fad51-c532-4e14-964d-42adc6c9ec3a"},"text/plain":"StatementMeta(, 83d43b1c-1c3b-49ed-92cf-5669aa60253c, 20, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: ratelimit in /nfs4/pyenv-9bf1c168-f8e2-40b7-9943-e0a4a046c6fb/lib/python3.11/site-packages (2.2.1)\r\n"]},{"output_type":"stream","name":"stdout","text":["\r\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\r\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\r\nNote: you may need to restart the kernel to use updated packages.\n"]},{"output_type":"stream","name":"stdout","text":["Warning: PySpark kernel has been restarted to use updated packages.\n\n"]}],"execution_count":1,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"jupyter":{"outputs_hidden":true}},"id":"aed8e2da-44a4-48eb-8817-f4134f98f61e"},{"cell_type":"code","source":["from datetime import datetime\n","import requests\n","import json\n","\n","# Get the most recent trade date in the Bronze lakehouse\n","query = \"select distinct refPer from bronze_hs2_ontario_us_trade_exports_data order by refPer desc\"\n","df = spark.sql(query)\n","value_lh = df.first()[\"refPer\"]\n","latest_date_lh = datetime.strptime(value_lh, '%Y-%m-%d').date()\n","\n","# Take the most recent trade date available by StatCan\n","product_id = 12100147\n","url = \"https://www150.statcan.gc.ca/t1/wds/rest/getCubeMetadata\"\n","post_body = [{\"productId\":product_id}]\n","\n","response = requests.post(url, json=post_body)\n","response.raise_for_status()\n","metadata = response.json()\n","\n","value_api = metadata[0][\"object\"][\"cubeEndDate\"]\n","latest_date_api = datetime.strptime(value_api, '%Y-%m-%d').date()\n","\n","# Set n_periods to the amount of months that were not called\n","n_periods = (latest_date_api.year - latest_date_lh.year) * 12 + (latest_date_api.month - latest_date_lh.month)\n","\n","print(f\"Most recent trade date in Lakehouse: {latest_date_lh}\")\n","print(f\"Most recent trade date available from StatsCan: {latest_date_api}\")\n","print(f\"Amount of months to call: {n_periods}\")\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":29,"statement_ids":[29],"state":"finished","livy_statement_state":"available","session_id":"83d43b1c-1c3b-49ed-92cf-5669aa60253c","normalized_state":"finished","queued_time":"2025-11-06T19:14:03.3099153Z","session_start_time":null,"execution_start_time":"2025-11-06T19:14:03.3110173Z","execution_finish_time":"2025-11-06T19:15:19.9148592Z","parent_msg_id":"78f3e634-bfe9-41b1-b843-70159abd2a9e"},"text/plain":"StatementMeta(, 83d43b1c-1c3b-49ed-92cf-5669aa60253c, 29, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Most recent trade date in Lakehouse: 2025-08-01\nMost recent trade date available from StatsCan: 2025-08-01\nAmount of months to call: 0\n"]}],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"advisor":{"adviceMetadata":"{\"artifactId\":\"f8c5c2dc-c2ea-41bc-8019-0d34b5968bc4\",\"activityId\":\"83d43b1c-1c3b-49ed-92cf-5669aa60253c\",\"applicationId\":\"application_1762453130491_0001\",\"jobGroupId\":\"29\",\"advices\":{\"info\":1}}"}},"id":"efaa10ad-404a-4544-a60e-46674ad7ddf7"},{"cell_type":"code","source":["import requests\n","import pandas as pd\n","from datetime import datetime, timezone\n","import os\n","from requests.adapters import HTTPAdapter\n","from urllib3.util.retry import Retry\n","import time\n","from concurrent.futures import ThreadPoolExecutor, as_completed\n","from unidecode import unidecode\n","from threading import Lock\n","from ratelimit import limits, sleep_and_retry\n","from pyspark.sql import SparkSession\n","from py4j.java_gateway import java_import\n","import time\n","\n","spark = SparkSession.builder.getOrCreate()\n","\n","# Config\n","range_hs2 = range(1, 100)   # Dimension 5 - HS2 (Product Code)\n","range_states = range(1, 55) # Dimension 3 - US States\n","post_url = \"https://www150.statcan.gc.ca/t1/wds/rest/getDataFromCubePidCoordAndLatestNPeriods\"\n","lakehouse_path = \"abfss://1dcd65a7-d5a3-4e2b-a110-db438703b7b5@onelake.dfs.fabric.microsoft.com/f8c35c71-7fa4-4e87-9b9a-91435e298fb7/Tables\"\n","product_ids = {\n","    \"exports\": 12100147, # Canadian international merchandise trade data cube product IDs\n","    \"imports\": 12100150\n","}\n","\n","# Check if lakehouse files exist\n","def table_exists(table_name):\n","    try:\n","        spark.table(table_name)\n","        return True\n","    except:\n","        return False\n","\n","# Retrieve and process JSON\n","@sleep_and_retry\n","@limits(calls=1, period=1) # Limit to one call per second\n","def get_recent_data(post_url: str, post_body: list, extra_col: tuple):\n","    trade_type = extra_col[3]\n","    product_id = post_body[0][\"productId\"]\n","\n","    # Make API request, extract JSON file\n","    session = get_session_with_retries()\n","    response = session.post(post_url, json=post_body, timeout=(5, 30))\n","    response.raise_for_status()\n","    json_data = response.json()\n","\n","    # List indexing and key searching for records\n","    records = json_data[0][\"object\"][\"vectorDataPoint\"] \n","\n","    # Add additional re-exports column for the exports file\n","    if trade_type == \"exports\":\n","        reexports_records = get_reexport(product_id, extra_col[2]) # Pull JSON with values for reexports instead of domestic exports\n","        \n","        # Populate domestic exports JSON with a reexport field\n","        if ((len(reexports_records) == len(records)) and (len(reexports_records) != 0) and (len(records) != 0)):\n","            for i in range(0, len(records)):\n","                value = reexports_records[i][\"value\"]\n","                records[i][\"reexport_value\"] = int(value) if value is not None else 0\n","\n","        # Populate reexports with 0 values if HS2/State combination does not have reexports but has domestic exports \n","        elif ((len(records) > len(reexports_records)) and (len(reexports_records) == 0)):\n","            print(\"No existing re-export data for coordinate \", post_body[0][\"coordinate\"], \" (Domestic: \", len(records), \" - Re-export: \", len(reexports_records), \")\")  \n","            for record in records:\n","                record[\"reexport_value\"] = 0\n","\n","        # Populate domestic exports with 0 values if HS2/State combination does not have domestic exports but has reexports\n","        elif ((len(reexports_records) > len(records)) and (len(records) == 0)):\n","            print(\"No existing domestic export data for coordinate \", post_body[0][\"coordinate\"], \" (Domestic: \", len(records), \" - Re-export: \", len(reexports_records), \")\")  \n","            records = reexports_records\n","            for record in records:\n","                record[\"reexport_value\"] = record[\"value\"]\n","                record[\"value\"] = 0\n","\n","        else:\n","            print(\"Differing amount of export data for domestic exports and re-exports for coordinate - \", post_body[0][\"coordinate\"], \" (Domestic: \", len(records), \" - Re-export: \", len(reexports_records), \")\")  \n","    \n","    # Convert to DataFrame, rename value column as suite\n","    df = pd.DataFrame.from_records(records)\n","    \n","    if trade_type == \"exports\":\n","        df.rename(columns={'value': 'domestic_export_value'}, inplace=True)\n","        value_col = \"domestic_export_value\"\n","    elif trade_type == \"imports\":\n","        df.rename(columns={'value': 'import_value'}, inplace=True)\n","        value_col = \"import_value\"\n","    else:\n","        value_col = \"value\"  # Fallback\n","\n","\n","    # Fill NaNs and cast to int safely\n","    for col in [value_col, \"reexport_value\"]:\n","        if col in df.columns:\n","            df[col] = df[col].fillna(0).astype(int)\n","\n","    # Rename value column name for better representation\n","    if trade_type == \"exports\":\n","        df.rename(columns={'value': 'domestic_export_value'}, inplace=True)\n","    elif trade_type == \"imports\":\n","        df.rename(columns={'value': 'import_value'}, inplace=True)\n","\n","    # Enrich with additional columns for info/metadata\n","    try:\n","        df[\"principal_trading_partner_state\"] = pull_values(extra_col[0], \"State\", product_id)\n","        df[\"hs2_commodity\"] = unidecode(pull_values(extra_col[1], \"HS2\", product_id))\n","        df[\"coordinate\"] = extra_col[2]\n","        # df[\"product_id\"] = post_body[0][\"productId\"]\n","        df[\"retrieved_timestamp\"] = datetime.now(timezone.utc).isoformat()\n","    except AttributeError:\n","        print(f\"No values for {extra_col} found for coordinate {extra_col[2]}\")\n","    return df\n","    \n","#---------------------------------------------------------------------------#\n","\n","# Pull reexport values for the exports table\n","def get_reexport(pid: int, coords: str):\n","    post_url = \"https://www150.statcan.gc.ca/t1/wds/rest/getDataFromCubePidCoordAndLatestNPeriods\"\n","\n","    # Take coordinate for the domestic exports of the HS2/State, turn the coordinate into its respective reexport version\n","    coords_split = coords.split(\".\")\n","    coords_split[3] = \"2\"\n","    new_coords = \".\".join(coords_split)\n","\n","    # Make API request, extract JSON file\n","    post_body = [{\"productId\": pid, \"coordinate\": new_coords, \"latestN\": n_periods}]\n","    session = get_session_with_retries()\n","    response = session.post(post_url, json=post_body, timeout=(5, 30))\n","    reexport_data = response.json()\n","\n","    # List indexing and key searching for records\n","    reexport_output = reexport_data[0][\"object\"][\"vectorDataPoint\"]\n","    \n","    return reexport_output\n","\n","#---------------------------------------------------------------------------#\n","\n","# Pull the name of the State/HS2 by referencing metadata\n","\n","metadata_lock = Lock()\n","def pull_values(member_id: int, field: str, product_id: int):\n","\n","    with metadata_lock:\n","        # Pull JSON of metadata if not pulled already\n","        if product_id not in metadata_cache:\n","            url = \"https://www150.statcan.gc.ca/t1/wds/rest/getCubeMetadata\"\n","            post_body = [{\"productId\":product_id}]\n","\n","            response = requests.post(url, json=post_body)\n","            data = response.json()\n","            metadata_cache[product_id] = response.json()\n","\n","    # Return HS2 commodity or state, depending on what is requested\n","    data = metadata_cache[product_id]\n","    for dimension in data[0][\"object\"][\"dimension\"]:\n","        if dimension[\"dimensionNameEn\"] == field:\n","            for member in dimension[\"member\"]:\n","                if member[\"memberId\"] == member_id:\n","                    return member[\"memberNameEn\"]\n","    \n","    return None\n","\n","#---------------------------------------------------------------------------#\n","\n","# Retry an API request in case of failure (adds a delay in case of rate limiting)\n","def get_session_with_retries():\n","    session = requests.Session()\n","    retries = Retry(\n","        total = 5,\n","        backoff_factor = 2, # Exponential backoff: 1s, 2s, 4s, etc.\n","        status_forcelist = [429, 502, 503, 504],\n","        allowed_methods = [\"POST\"],\n","        raise_on_status = False\n","    )\n","    adapter = HTTPAdapter(max_retries=retries)\n","    session.mount(\"https://\", adapter)\n","    return session\n","\n","#---------------------------------------------------------------------------#\n","\n","write_lock = Lock()\n","\n","# Call get_recent_data() to get the Dataframe for the inputted coordinate, append to lakehouse table\n","def process_request(trade_type, pid, state, hs2):\n","    # Set coordinates based on input\n","    coordinates = {\n","        \"exports\": f\"35.9.{state}.1.{hs2}.0.0.0.0.0\",\n","        \"imports\": f\"35.9.{state}.1.{hs2}.0.0.0.0.0\"\n","    }\n","\n","    post_body = [{\"productId\": pid, \"coordinate\": coordinates[trade_type], \"latestN\": n_periods}]\n","    df = get_recent_data(post_url, post_body, (state, hs2, coordinates[trade_type], trade_type))\n","    output_path = f\"{lakehouse_path}/bronze_hs2_ontario_us_trade_{trade_type}_data\"\n","\n","    with write_lock:\n","        # If no data was extracted for a specific coordinate, append the coordinate to a list to try and call later\n","        if df.empty and hs2 != 77: # No HS2 77 code\n","            print(f\"No data for {trade_type}, state {state}, HS2 {hs2}\")\n","            missed_coords.append((trade_type, pid, state, hs2))\n","            return \"Error\"\n","\n","        # Turn pandas DataFrame into a spark DataFrame\n","        else:\n","            try:\n","                spark_df = spark.createDataFrame(df)\n","            except Exception as e:\n","                print(f\"Error converting [{trade_type} - State: {state} - {hs2}] to DataFrame: {e}\")\n","                return \"Error\"\n","\n","        table_name = f\"bronze_hs2_ontario_us_trade_{trade_type}_data\"\n","\n","        # Append data to table, or create a new one if table does not exist\n","        if table_exists(table_name):\n","            spark_df.write.format(\"delta\").option(\"mergeSchema\", \"true\").mode(\"append\").saveAsTable(table_name)\n","        else:\n","            spark_df.write.format(\"delta\").option(\"overwriteSchema\", \"true\").mode(\"overwrite\").saveAsTable(table_name)\n","\n","    return f\"Successfully saved {trade_type} data (State: {state}, HS2: {hs2}) to {output_path}\"\n","\n","#---------------------------------------------------------------------------#\n","\n","# Retrieve and filter for Ontario-U.S.\n","if __name__ == \"__main__\":\n","    metadata_cache = {}\n","    written_files = set()\n","    missed_coords = []\n","\n","    start_time = time.time()\n","\n","    # Multithreading API calls for parallel calls\n","    with ThreadPoolExecutor(max_workers=2) as executor:\n","        futures = []\n","\n","        # Iterate through coordinates for different State/HS2 for imports and exports\n","        for trade_type, pid in product_ids.items():\n","            for state in range_states:\n","                for hs2 in range_hs2:\n","                    futures.append(executor.submit(process_request, trade_type, pid, state, hs2))\n","                    \n","        for future in as_completed(futures):\n","            print(future.result())\n","    \n","        print(f\"Coordinates not written in initial call: {missed_coords}\")\n","        \n","        # Recall any coordinates that did not produce an output\n","        missed_coords_copy = list(missed_coords)\n","\n","        for coord in missed_coords_copy:\n","            status = process_request(coord[0], coord[1], coord[2], coord[3]) \n","           \n","    end_time = time.time()\n","    time_elapsed = int(end_time - start_time)\n","\n","    print(f\"Completed in {time_elapsed} second(s)\")\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":23,"statement_ids":[23],"state":"finished","livy_statement_state":"available","session_id":"83d43b1c-1c3b-49ed-92cf-5669aa60253c","normalized_state":"finished","queued_time":"2025-11-06T18:50:56.0370369Z","session_start_time":null,"execution_start_time":"2025-11-06T18:52:44.5284193Z","execution_finish_time":"2025-11-06T18:52:44.9174476Z","parent_msg_id":"0ee4704d-e410-41f6-90c8-b9ec57e54410"},"text/plain":"StatementMeta(, 83d43b1c-1c3b-49ed-92cf-5669aa60253c, 23, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["No new data to append\nLatest date of data available: 2025-08-01\nLatest date of data currently in lakehouse: 2025-08-01\n"]}],"execution_count":2,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"6d9803c7-3faa-4006-81a7-fb0971ef7ba1"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","display_name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"known_lakehouses":[{"id":"f8c35c71-7fa4-4e87-9b9a-91435e298fb7"}],"default_lakehouse":"f8c35c71-7fa4-4e87-9b9a-91435e298fb7","default_lakehouse_name":"lh_bronze_ontario_us_trade_data_storing","default_lakehouse_workspace_id":"1dcd65a7-d5a3-4e2b-a110-db438703b7b5"}}},"nbformat":4,"nbformat_minor":5}