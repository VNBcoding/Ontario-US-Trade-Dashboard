{"cells":[{"cell_type":"code","source":["import requests, zipfile, io, re, pandas as pd\n","from pyspark.sql import SparkSession\n","\n","# Configuration\n","spark = SparkSession.builder.getOrCreate()\n","lakehouse_path = \"abfss://1dcd65a7-d5a3-4e2b-a110-db438703b7b5@onelake.dfs.fabric.microsoft.com/f8c35c71-7fa4-4e87-9b9a-91435e298fb7/Files\"\n","language = \"en\"\n","post_url = \"https://www150.statcan.gc.ca/t1/wds/rest/getFullTableDownloadCSV\"\n","datasets = {\n","    \"12100175\": \"trade_by_province\",\n","    \"12100011\": \"trade_by_country\",  \n","    \"12100163\": \"trade_by_commodity\",\n","    \"12100168\": \"trade_price_volume_indices\"\n","}\n","\n","# Download ZIP folder from StatCan WDS API\n","def download_dataset(dataset_id: str) -> io.BytesIO:\n","    print(\"1. Navigating to StatCan WDS API\")\n","    api_url = f\"{post_url}/{dataset_id}/{language}\"\n","    api_resp = requests.get(api_url)\n","    api_resp.raise_for_status()\n","\n","    print(\"\\t1.1. Fetching ZIP folder download link\")\n","    zip_folder_download_link = api_resp.json().get(\"object\")\n","    if not zip_folder_download_link:\n","        raise ValueError(f\"Error: No 'object' property found for dataset {dataset_id}.\")\n","\n","    print(f\"\\t1.2. Downloading ZIP folder: {zip_folder_download_link}\")\n","    zip_folder_download_resp = requests.get(zip_folder_download_link, stream=True)\n","    zip_folder_download_resp.raise_for_status()\n","    return io.BytesIO(zip_folder_download_resp.content)\n","\n","# Extract and filter CSV file from ZIP folder\n","def filter_dataset(zip_folder_bytes: io.BytesIO, dataset_id) -> pd.DataFrame:\n","    print(\"2. Extracting CSV file from ZIP folder\")\n","    with zipfile.ZipFile(zip_folder_bytes) as zip_folder:\n","        csv_filename = next(\n","            filename for filename in zip_folder.namelist()\n","            if filename.endswith(\".csv\") and \"MetaData\" not in filename\n","        )\n","        \n","        print(f\"\\t2.1. Filtering CSV file: {csv_filename}\")\n","        filtered_chunks = []\n","        for chunk in pd.read_csv(zip_folder.open(csv_filename), chunksize=100_000, low_memory=False):\n","            period_column = next((column for column in chunk.columns if \"ref_date\" in column.lower()), None)\n","            principal_trading_partner_column = next((column for column in chunk.columns if \"principal trading partners\" in column.lower()), None)\n","            \n","            if period_column:\n","                chunk[period_column] = pd.to_datetime(chunk[period_column], errors=\"coerce\")\n","                if (principal_trading_partner_column and dataset_id in ['12100175', '12100011']):\n","                    filtered_chunk = chunk[\n","                        (chunk[period_column] >= \"2020-01-01\") &\n","                        (chunk[principal_trading_partner_column] == \"United States\")\n","                    ]\n","                else:\n","                    filtered_chunk = chunk[chunk[period_column] >= \"2020-01-01\"]\n","                if not filtered_chunk.empty:\n","                    filtered_chunks.append(filtered_chunk)\n","\n","        if not filtered_chunks:\n","            print(f\"Error: No matching rows found in dataset.\")\n","            return pd.DataFrame()\n","        \n","        df_filtered = pd.concat(filtered_chunks, ignore_index=True)\n","        df_filtered = clean_dataset_column_names(df_filtered)\n","        print(f\"\\t2.2. Retaining {len(df_filtered):,} filtered rows\")\n","        return df_filtered\n","\n","def clean_dataset_column_names(df: pd.DataFrame) -> pd.DataFrame:\n","    cleaned_columns = []\n","    for column in df.columns:\n","        # Replace invalid characters with underscore\n","        cleaned = re.sub(r'[ ,;{}()\\n\\t=]', '_', column)\n","        cleaned = re.sub(r'_+', '_', cleaned)\n","        cleaned = cleaned.strip('_')\n","        cleaned_columns.append(cleaned)\n","    df.columns = cleaned_columns\n","    return df\n","\n","# Save dataset to Lakehouse Files and Tables folders (bronze layer)\n","def save_dataset_to_lakehouse(df: pd.DataFrame, dataset_id: str, dataset_description: str):\n","    if df.empty:\n","        print(f\"Error: No rows found in dataset. Skipping.\")\n","        return\n","\n","    print(f\"3. Saving dataset to Bronze layer Lakehouse\")\n","    spark_df = spark.createDataFrame(df)\n","\n","    # Save to Files folder as parquet\n","    save_path = f\"{lakehouse_path}/bronze_napcs_ontario_us_{dataset_description}_data\"\n","    spark_df.repartition(2).write.mode(\"overwrite\").parquet(save_path)\n","\n","    # Save to Tables folder\n","    table_name = f\"bronze_napcs_ontario_us_{dataset_description}_data\"\n","    spark_df.write.mode(\"overwrite\").saveAsTable(table_name)\n","\n","# Main loop\n","for dataset_id, dataset_description in datasets.items():\n","    try:\n","        print(f\"\\nStarting dataset {dataset_id} ingestion:\")\n","        zip_folder_bytes = download_dataset(dataset_id)\n","        df_filtered = filter_dataset(zip_folder_bytes, dataset_id)\n","        save_dataset_to_lakehouse(df_filtered, dataset_id, dataset_description)\n","        print(f\"Succeeded\")\n","    except Exception as e:\n","        print(f\"Error processing dataset {dataset_id}: {e}\")\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":3,"statement_ids":[3],"state":"finished","livy_statement_state":"available","session_id":"dc0a92e6-e24e-46e5-ad53-2e9eab7ccc29","normalized_state":"finished","queued_time":"2025-11-05T03:30:27.5429354Z","session_start_time":"2025-11-05T03:30:27.544028Z","execution_start_time":"2025-11-05T03:30:50.8325437Z","execution_finish_time":"2025-11-05T03:32:26.195442Z","parent_msg_id":"fd3f10e0-bb64-4461-86a2-bbba2e742b27"},"text/plain":"StatementMeta(, dc0a92e6-e24e-46e5-ad53-2e9eab7ccc29, 3, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\nStarting dataset 12100175 ingestion:\n1. Navigating to StatCan WDS API\n\t1.1. Fetching ZIP folder download link\n\t1.2. Downloading ZIP folder: https://www150.statcan.gc.ca/n1/tbl/csv/12100175-eng.zip\n2. Extracting CSV file from ZIP folder\n\t2.1. Filtering CSV file: 12100175.csv\n\t2.2. Retaining 25,636 filtered rows\n3. Saving dataset to Bronze layer Lakehouse\nSucceeded\n\nStarting dataset 12100011 ingestion:\n1. Navigating to StatCan WDS API\n\t1.1. Fetching ZIP folder download link\n\t1.2. Downloading ZIP folder: https://www150.statcan.gc.ca/n1/tbl/csv/12100011-eng.zip\n2. Extracting CSV file from ZIP folder\n\t2.1. Filtering CSV file: 12100011.csv\n\t2.2. Retaining 612 filtered rows\n3. Saving dataset to Bronze layer Lakehouse\nSucceeded\n\nStarting dataset 12100163 ingestion:\n1. Navigating to StatCan WDS API\n\t1.1. Fetching ZIP folder download link\n\t1.2. Downloading ZIP folder: https://www150.statcan.gc.ca/n1/tbl/csv/12100163-eng.zip\n2. Extracting CSV file from ZIP folder\n\t2.1. Filtering CSV file: 12100163.csv\n\t2.2. Retaining 82,416 filtered rows\n3. Saving dataset to Bronze layer Lakehouse\nSucceeded\n\nStarting dataset 12100168 ingestion:\n1. Navigating to StatCan WDS API\n\t1.1. Fetching ZIP folder download link\n\t1.2. Downloading ZIP folder: https://www150.statcan.gc.ca/n1/tbl/csv/12100168-eng.zip\n2. Extracting CSV file from ZIP folder\n\t2.1. Filtering CSV file: 12100168.csv\n\t2.2. Retaining 247,248 filtered rows\n3. Saving dataset to Bronze layer Lakehouse\nSucceeded\n"]}],"execution_count":1,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"022c9792-968b-4f94-a914-d9786177286d"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","display_name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"known_lakehouses":[{"id":"f8c35c71-7fa4-4e87-9b9a-91435e298fb7"}],"default_lakehouse":"f8c35c71-7fa4-4e87-9b9a-91435e298fb7","default_lakehouse_name":"lh_bronze_ontario_us_trade_data_storing","default_lakehouse_workspace_id":"1dcd65a7-d5a3-4e2b-a110-db438703b7b5"}}},"nbformat":4,"nbformat_minor":5}